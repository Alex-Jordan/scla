<?xml version="1.0" encoding="UTF-8" ?>
<section acro="NLT">
<title>Nilpotent Linear Transformations</title>

<introduction></introduction>
<subsection>
<!-- %%%%%%%%%% -->
<!-- % -->
<!-- %  Section NLT -->
<!-- %  Nilpotent Linear Transformations -->
<!-- % -->
<!-- %%%%%%%%%% -->
{\sc\large This section is in draft form}\\
{\sc\large Nearly complete}
<p><p>\medskip

\subsect{NLT}{Nilpotent Linear Transformations}
We will discover that nilpotent linear transformations are the essential obstacle in a non-diagonalizable linear transformation.  So we will study them carefully first, both as an object of inherent mathematical interest, but also as the object at the heart of the argument that leads to a pleasing canonical form for any linear transformation.  Once we understand these linear transformations thoroughly, we will be able to easily analyze the structure of any linear transformation.<p><p>
<definition acro="NLT" index="nilpotent!linear transformation">
<title>Nilpotent Linear Transformation</title><p>
Suppose that <m>\ltdefn{T}{V}{V}</m> is a linear transformation such that there is an integer <m>p>0</m> such that <m>\lt{T^p}{\vect{v}}=\zerovector</m> for every <m>\vect{v}\in V</m>.  The smallest <m>p</m> for which this condition is met is called the <m><term>index</term></m> of <m>T</m>.
</p></definition>

Of course, the linear transformation <m>T</m> <em>defined</em> by <m>\lt{T}{\vect{v}}=\zerovector</m> will qualify as nilpotent of index 1.  But are there others?
<example acro="NM64" index="nilpotent!matrix!size 6, index 4">
<title>Nilpotent matrix, size 6, index 4</title>

Recall that our definitions and theorems are being stated for linear transformations on abstract vector spaces, while our examples will work with square matrices (and use the same terms interchangeably).  In this case, to demonstrate the existence of nontrivial nilpotent linear transformations, we desire a matrix such that some power of the matrix is the zero matrix.  Consider
<me><md>
<mrow>
A
</mrow>
<mrow>&amp;=]]>
\begin{bmatrix}
<![CDATA[ -3 & 3 & -2 & 5 & 0 & -5 \\]]>
<![CDATA[ -3 & 5 & -3 & 4 & 3 & -9 \\]]>
<![CDATA[ -3 & 4 & -2 & 6 & -4 & -3 \\]]>
<![CDATA[ -3 & 3 & -2 & 5 & 0 & -5 \\]]>
<![CDATA[ -3 & 3 & -2 & 4 & 2 & -6 \\]]>
<![CDATA[ -2 & 3 & -2 & 2 & 4 & -7]]>
\end{bmatrix}
<intertext>and compute powers of <m>A</m>,</intertext>
A^2
</mrow>
<mrow>&amp;=]]>
\begin{bmatrix}
<![CDATA[ 1 & -2 & 1 & 0 & -3 & 4 \\]]>
<![CDATA[ 0 & -2 & 1 & 1 & -3 & 4 \\]]>
<![CDATA[ 3 & 0 & 0 & -3 & 0 & 0 \\]]>
<![CDATA[ 1 & -2 & 1 & 0 & -3 & 4 \\]]>
<![CDATA[ 0 & -2 & 1 & 1 & -3 & 4 \\]]>
<![CDATA[ -1 & -2 & 1 & 2 & -3 & 4]]>
\end{bmatrix}\\
A^3
</mrow>
<mrow>&amp;=]]>
\begin{bmatrix}
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0 \\]]>
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0 \\]]>
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0 \\]]>
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0]]>
\end{bmatrix}\\
A^4
</mrow>
<mrow>&amp;=]]>
\begin{bmatrix}
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}\\
</mrow>
</md></me>
Thus we can say that <m>A</m> is nilpotent of index 4.<p><p>
Because it will presage some upcoming theorems, we will record some extra information about the eigenvalues and eigenvectors of <m>A</m> here.  <m>A</m> has just one eigenvalue, <m>\lambda=0</m>, with algebraic multiplicity <m>6</m> and geometric multiplicity <m>2</m>.  The eigenspace for this eigenvalue is
<me><md>
<mrow>
\eigenspace{A}{0}
</mrow>
<mrow>&amp;=]]>
\spn{
\colvector{2 \\ 2 \\ 5 \\ 2 \\ 1 \\ 0},\,
\colvector{-1 \\ -1 \\ -5 \\ -1 \\ 0 \\ 1}
}
</mrow>
</md></me>
If there were degrees of singularity, we might say this matrix was <em>very</em> singular, since zero is an eigenvalue with maximum algebraic multiplicity (<acroref type="theorem" acro="SMZE" />, <acroref type="theorem" acro="ME" />).  Notice too that <m>A</m> is <q>far</q> from being diagonalizable (<acroref type="theorem" acro="DMFE" />).
</example>

Another example.
<example acro="NM62" index="nilpotent!matrix!size 6, index 2">
<title>Nilpotent matrix, size 6, index 2</title>

Consider the matrix
<me><md>
<mrow>
B
</mrow>
<mrow>&amp;=]]>
\begin{bmatrix}
<![CDATA[ -1 & 1 & -1 & 4 & -3 & -1 \\]]>
<![CDATA[ 1 & 1 & -1 & 2 & -3 & -1 \\]]>
<![CDATA[ -9 & 10 & -5 & 9 & 5 & -15 \\]]>
<![CDATA[ -1 & 1 & -1 & 4 & -3 & -1 \\]]>
<![CDATA[ 1 & -1 & 0 & 2 & -4 & 2 \\]]>
<![CDATA[ 4 & -3 & 1 & -1 & -5 & 5]]>
\end{bmatrix}
<intertext>and compute the second power of <m>B</m>,</intertext>
B^2
</mrow>
<mrow>&amp;=]]>
\begin{bmatrix}
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}\\
</mrow>
</md></me>
So <m>B</m> is nilpotent of index 2.  Again, the only eigenvalue of <m>B</m> is zero, with algebraic multiplicity <m>6</m>.  The geometric multiplicity of the eigenvalue is <m>3</m>, as seen in the eigenspace,
<me><md>
<mrow>
\eigenspace{B}{0}
</mrow>
<mrow>&amp;=]]>
\spn{
\colvector{1 \\ 3 \\ 6 \\ 1 \\ 0 \\ 0},\,
\colvector{0 \\ -4 \\ -7 \\ 0 \\ 1 \\ 0},\,
\colvector{0 \\ 2 \\ 1 \\ 0 \\ 0 \\ 1}
}
</mrow>
</md></me>
Again, <acroref type="theorem" acro="DMFE" /> tells us that <m>B</m> is far from being diagonalizable.
</example>

On a first encounter with the definition of a nilpotent matrix, you might wonder if such a thing was possible at all.  That a high power of a nonzero object could be zero is so very different from our experience with scalars
<!-- % TODO:  ref scalar product theorem -->
that it seems very unnatural.  Hopefully the two previous examples were somewhat surprising.  But we have seen that matrix algebra does not always behave the way we expect (<acroref type="example" acro="MMNC" />), and we also now recognize matrix products not just as arithmetic, but as function composition (<acroref type="theorem" acro="MRCLT" />).  We will now turn to some examples of nilpotent matrices which might be more transparent.
<definition acro="JB" index="Jordan block">
<title>Jordan Block</title><p>
Given the scalar <m>\lambda\in\complexes</m>, the Jordan block <m>\jordan{n}{\lambda}</m> is the <m>n\times n</m> matrix defined by
<me><md>
<mrow>
\matrixentry{\jordan{n}{\lambda}}{ij}
</mrow>
<mrow>&amp;=]]>
\begin{cases}
<![CDATA[\lambda & i=j\\]]>
<![CDATA[1 & j=i+1\\]]>
<![CDATA[0 & \text{otherwise}]]>
\end{cases}
</mrow>
</md></me>
<notation acro="JB" index="Jordan block">
<title>Jordan Block</title>
<usage><m>\jordan{n}{\lambda}</m></usage>
</notation>
</p></definition>

<example acro="JB4" index="Jordan block!size 4">
<title>Jordan block, size 4</title>

A simple example of a Jordan block,
<me><md>
<mrow>
<![CDATA[\jordan{4}{5}&=]]>
\begin{bmatrix}
<![CDATA[5 & 1 & 0 & 0\\]]>
<![CDATA[0 & 5 & 1 & 0\\]]>
<![CDATA[0 & 0 & 5 & 1\\]]>
<![CDATA[0 & 0 & 0 & 5]]>
\end{bmatrix}
</mrow>
</md></me>
</example>

We will return to general Jordan blocks later, but in this section we are just interested in Jordan blocks where <m>\lambda=0</m>.  Here's an example of why we are specializing in these matrices now.
<example acro="NJB5" index="Jordan block!nilpotent!size 5">
<title>Nilpotent Jordan block, size 5</title>

Consider
<me><md>
<mrow>
<![CDATA[\jordan{5}{0}&=]]>
\begin{bmatrix}
<![CDATA[0 & 1 & 0 & 0 & 0\\]]>
<![CDATA[0 & 0 & 1 & 0 & 0\\]]>
<![CDATA[0 & 0 & 0 & 1 & 0\\]]>
<![CDATA[0 & 0 & 0 & 0 & 1\\]]>
<![CDATA[0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}\\
<intertext>and compute powers,</intertext>
<![CDATA[\left(\jordan{5}{0}\right)^2&=]]>
\begin{bmatrix}
<![CDATA[ 0 & 0 & 1 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 1 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 1 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}\\
<![CDATA[\left(\jordan{5}{0}\right)^3&=]]>
\begin{bmatrix}
<![CDATA[ 0 & 0 & 0 & 1 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 1 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}\\
<![CDATA[\left(\jordan{5}{0}\right)^4&=]]>
\begin{bmatrix}
<![CDATA[ 0 & 0 & 0 & 0 & 1 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}\\
<![CDATA[\left(\jordan{5}{0}\right)^5&=]]>
\begin{bmatrix}
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}\\
</mrow>
</md></me>
So <m>\jordan{5}{0}</m> is nilpotent of index 5.  As before, we record some information about the eigenvalues and eigenvectors of this matrix.  The only eigenvalue is zero, with algebraic multiplicity 5, the maximum possible (<acroref type="theorem" acro="ME" />).  The geometric multiplicity of this eigenvalue is just 1, the minimum possible (<acroref type="theorem" acro="ME" />), as seen in the eigenspace,
<me><md>
<mrow>
<![CDATA[\eigenspace{\jordan{5}{0}}{0}&=]]>
\spn{\colvector{1 \\ 0 \\ 0 \\ 0 \\ 0}}
</mrow>
</md></me>
There should not be any real surprises in this example.  We can watch the ones in the powers of <m>\jordan{5}{0}</m> slowly march off to the upper-right hand corner of the powers.  In some vague way, the eigenvalues and eigenvectors of this matrix are equally extreme.
</example>

We can form combinations of Jordan blocks to build a variety of nilpotent matrices.  Simply place Jordan blocks on the diagonal of a matrix with zeros everywhere else, to create a <term>block diagonal</term> matrix.
<example acro="NM83" index="nilpotent matrix!size 8! index 3">
<title>Nilpotent matrix, size 8, index 3</title>

Consider the matrix
<me><md>
<mrow>
<![CDATA[C&=]]>
\begin{bmatrix}
<![CDATA[\jordan{3}{0} & \zeromatrix & \zeromatrix \\]]>
<![CDATA[\zeromatrix & \jordan{3}{0} & \zeromatrix \\]]>
<![CDATA[\zeromatrix & \zeromatrix & \jordan{2}{0}]]>
\end{bmatrix}
=
\begin{bmatrix}
<![CDATA[ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}\\
<intertext>and compute powers,</intertext>
<![CDATA[C^2&=]]>
\begin{bmatrix}
<![CDATA[ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}\\
<![CDATA[C^3&=]]>
\begin{bmatrix}
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}
</mrow>
</md></me>
So <m>C</m> is nilpotent of index 3.  You should notice how block diagonal matrices behave in products (much like diagonal matrices) and that it was the largest Jordan block that determined the index of this combination.  All eight eigenvalues are zero, and each of the three Jordan blocks contributes one eigenvector to a basis for the eigenspace, resulting in zero having a geometric multiplicity of 3.
</example>

It would appear that nilpotent matrices only have zero as an eigenvalue, so the algebraic multiplicity will be the maximum possible.  However, by creating block diagonal matrices with Jordan blocks on the diagonal you should be able to attain any desired geometric multiplicity for this lone eigenvalue.  Likewise, the size of the largest Jordan block employed will determine the index of the matrix. So nilpotent matrices with various combinations of index and geometric multiplicities are easy to manufacture.  The predictable properties of block diagonal matrices in matrix products and eigenvector computations, along with the next theorem, make this possible.  You might find <acroref type="example" acro="NJB5" /> a useful companion to this proof.
<theorem acro="NJB" index="Jordan block!nilpotent">
<title>Nilpotent Jordan Blocks</title>
<statement>
The Jordan block <m>\jordan{n}{0}</m> is nilpotent of index <m>n</m>.
</statement>

<proof>
While not phrased as an if-then statement, the statement in the theorem is understood to mean that if we have a specific matrix (<m>\jordan{n}{0}</m>) then we need to establish it is nilpotent of a specified index.  The first column of <m>\jordan{n}{0}</m> is the zero vector, and the remaining <m>n-1</m> columns are the standard unit vectors <m>\vect{e}_i</m>, <m>1\leq i\leq n-1</m> (<acroref type="definition" acro="SUV" />), which are also the first <m>n-1</m> columns of the size <m>n</m> identity matrix <m>I_n</m>.  As shorthand, write <m>J=\jordan{n}{0}</m>.
<me><md>
<mrow>
<![CDATA[J&=]]>
\left[\zerovector\left|\vect{e}_1\right.\left|\vect{e}_2\right.\left|\vect{e}_3\right.\left|\dots\right.\left|\vect{e}_{n-1}\right.\right]
</mrow>
</md></me>
We will use the definition of matrix multiplication (<acroref type="definition" acro="MM" />), together with a proof by induction (<acroref type="technique" acro="I" />), to study the powers of <m>J</m>.    Our claim is that
<me><md>
<mrow>
<![CDATA[J^k&=]]>
\left[\zerovector\left|\zerovector\right.\left|\dots\right.\left|\zerovector\right.\left|\vect{e}_1\right.\left|\vect{e}_2\right.\left|\dots\right.\left|\vect{e}_{n-k}\right.\right]
</mrow>
</md></me>
for <m>1\leq k\leq n</m>.  For the base case, <m>k=1</m>, and the definition of <m>J^1=\jordan{n}{0}</m> establishes the claim.  For the induction step, first note that <m>J\vect{e_1}=\zerovector</m> and <m>J\vect{e}_i=\vect{e}_{i-1}</m> for <m>2\leq i\leq n</m>.  Then, assuming the claim is true for <m>k</m>, we examine the <m>k+1</m> case,
<me><md>
<mrow>
J^{k+1}
</mrow>
<mrow>&amp;=JJ^k\\]]>
</mrow>
<mrow>&amp;=J\left[\zerovector\left|\zerovector\right.\left|\dots\right.\left|\zerovector\right.\left|\vect{e}_1\right.\left|\vect{e}_2\right.\left|\dots\right.\left|\vect{e}_{n-k}\right.\right]]]>
</mrow>
<mrow>&amp;&\text{Induction Hypothesis}\\]]>
</mrow>
<mrow>&amp;=\left[J\zerovector\left|J\zerovector\right.\left|\dots\right.\left|J\zerovector\right.\left|J\vect{e}_1\right.\left|J\vect{e}_2\right.\left|\dots\right.\left|J\vect{e}_{n-k}\right.\right]&amp;&amp;<acroref type="definition" acro="MM" />
</mrow>
<mrow>&amp;=\left[\zerovector\left|\zerovector\right.\left|\dots\right.\left|\zerovector\right.\left|\zerovector\right.\left|\vect{e}_1\right.\left|\vect{e}_2\right.\left|\dots\right.\left|\vect{e}_{n-k-1}\right.\right]&amp;&amp;<acroref type="definition" acro="MVP" />
</mrow>
<mrow>&amp;=\left[\zerovector\left|\zerovector\right.\left|\dots\right.\left|\zerovector\right.\left|\vect{e}_1\right.\left|\vect{e}_2\right.\left|\dots\right.\left|\vect{e}_{n-(k+1)}\right.\right]]]>
</mrow>
</md></me>
This concludes the induction.  So <m>J^k</m> has a nonzero entry (a one) in row <m>n-k</m> and column <m>n</m>, for <m>1\leq k\leq n-1</m>, and is therefore a nonzero matrix.  However, <m>J^n=\left[\zerovector\left|\zerovector\right.\left|\dots\right.\left|\zerovector\right.\right]=\zeromatrix</m>.  By <acroref type="definition" acro="NLT" />, <m>J</m> is nilpotent of index <m>n</m>.
</proof>
</theorem>

\subsect{PNLT}{Properties of Nilpotent Linear Transformations}
In this subsection we collect some basic properties of nilpotent linear transformations.  After studying the examples in the previous section, some of these will be no surprise.
<theorem acro="ENLT" index="nilpotent!linear transformation!eigenvalues">
<title>Eigenvalues of Nilpotent Linear Transformations</title>
<statement>
Suppose that <m>\ltdefn{T}{V}{V}</m> is a nilpotent linear transformation and <m>\lambda</m> is an eigenvalue of <m>T</m>.  Then <m>\lambda=0</m>.
</statement>

<proof>
Let <m>\vect{x}</m> be an eigenvector of <m>T</m> for the eigenvalue <m>\lambda</m>, and suppose that <m>T</m> is nilpotent with index <m>p</m>.  Then
<me><md>
<mrow>
\zerovector
</mrow>
<mrow>&amp;=\lt{T^p}{\vect{x}}&amp;&amp;<acroref type="definition" acro="NLT" />
</mrow>
<mrow>&amp;=\lambda^p\vect{x}&amp;&amp;<acroref type="theorem" acro="EOMP" />
</mrow>
</md></me>
Because <m>\vect{x}</m> is an eigenvector, it is nonzero, and therefore <acroref type="theorem" acro="SMEZV" /> tells us that <m>\lambda^p=0</m> and so <m>\lambda=0</m>.
</proof>
</theorem>

Paraphrasing, all of the eigenvalues of a nilpotent linear transformation are zero.  So in particular, the characteristic polynomial of a nilpotent linear transformation, <m>T</m>, on a vector space of dimension <m>n</m>, is simply <m>\charpoly{T}{x}=x^n</m>.<p><p>
<!-- % TODO:  Later get the converse from JCF, Cayley-Hamilton, and reference it here. -->
The next theorem is not critical for what follows, but it will explain our interest in nilpotent linear transformations.  More specifically, it is the first step in backing up the assertion that nilpotent linear transformations are the essential obstacle in a non-diagonalizable linear transformation.  While it is not obvious from the statement of the theorem, it says that a nilpotent linear transformation is not diagonalizable, unless it is trivially so.
<theorem acro="DNLT" index="nilpotent!linear transformation!diagonalizable">
<title>Diagonalizable Nilpotent Linear Transformations</title>
<statement>
Suppose the linear transformation <m>\ltdefn{T}{V}{V}</m> is nilpotent.   Then <m>T</m> is diagonalizable if and only if <m>T</m> is the zero linear transformation.
</statement>

<proof>
We start with the easy direction.  Let <m>n=\dimension{V}</m>.<p><p>
<implyreverse />\quad The linear transformation <m>\ltdefn{Z}{V}{V}</m> defined by <m>\lt{Z}{\vect{v}}=\zerovector</m> for all <m>\vect{v}\in V</m> is nilpotent of index <m>p=1</m> and a matrix representation relative to any basis of <m>V</m> is the <m>n\times n</m> zero matrix, <m>\zeromatrix</m>.  Quite obviously, the zero matrix is a diagonal matrix (<acroref type="definition" acro="DIM" />) and hence <m>Z</m> is diagonalizable (<acroref type="definition" acro="DZM" />).<p><p>
<implyforward />\quad Assume now that <m>T</m> is diagonalizable, so <m>\geomult{T}{\lambda}=\algmult{T}{\lambda}</m> for every eigenvalue <m>\lambda</m> (<acroref type="theorem" acro="DMFE" />).  By <acroref type="theorem" acro="ENLT" />, <m>T</m> has only one eigenvalue (zero), which therefore must have algebraic multiplicity <m>n</m>  (<acroref type="theorem" acro="NEM" />).  So the geometric multiplicity of zero will be <m>n</m> as well, <m>\geomult{T}{0}=n</m>.<p><p>
Let <m>B</m> be a basis for the eigenspace <m>\eigenspace{T}{0}</m>.  Then <m>B</m> is a linearly independent subset of <m>V</m> of size <m>n</m>, and by <acroref type="theorem" acro="G" /> will be a basis for <m>V</m>.  For any <m>\vect{x}\in B</m> we have
<me><md>
<mrow>
\lt{T}{\vect{x}}
</mrow>
<mrow>&amp;=0\vect{x}&amp;&amp;<acroref type="definition" acro="EM" />
</mrow>
<mrow>&amp;=\zerovector&amp;&amp;<acroref type="theorem" acro="ZSSM" />
</mrow>
</md></me>
So <m>T</m> is identically zero on a basis for <m>B</m>, and since the action of a linear transformation on a basis determines all of the values of the linear transformation (<acroref type="theorem" acro="LTDB" />), it is easy to see that <m>\lt{T}{\vect{v}}=\zerovector</m> for every <m>\vect{v}\in V</m>.
</proof>
</theorem>

So, other than one trivial case (the zero matrix), every nilpotent linear transformation is not diagonalizable.  It remains to see what is so <q>essential</q> about this broad class of non-diagonalizable linear transformations.  For this we now turn to a discussion of kernels of powers of nilpotent linear transformations, beginning with a result about general linear transformations that may not necessarily be nilpotent.


We now specialize <acroref type="theorem" acro="KPLT" /> to the case of nilpotent linear transformations, which buys us just a bit more precision in the conclusion.
<theorem acro="KPNLT" index="linear transformation!nilpotent!kernels of powers">
<title>Kernels of Powers of Nilpotent Linear Transformations</title>
<statement>
Suppose <m>\ltdefn{T}{V}{V}</m> is a nilpotent linear transformation with index <m>p</m> and <m>\dimension{V}=n</m>.  Then <m>0\leq p\leq n</m> and
<me><md>
<mrow>
\set{\zerovector}
</mrow>
<mrow>&amp;=\krn{T^0}]]>
\subsetneq\krn{T^1}
\subsetneq\krn{T^2}
\subsetneq\cdots
\subsetneq\krn{T^{p}}
=\krn{T^{p+1}}
=\cdots
=V
</mrow>
</md></me>
</statement>

<proof>
Since <m>T^p=0</m> it follows that <m>T^{p+j}=0</m> for all <m>j\geq 0</m> and thus <m>\krn{T^{p+j}}=V</m> for <m>j\geq 0</m>.  So the value of <m>m</m> guaranteed by <acroref type="theorem" acro="KPLT" /> is at most <m>p</m>.  The only remaining aspect of our conclusion that does not follow from <acroref type="theorem" acro="KPLT" /> is that <m>m=p</m>.  To see this we must show that $\krn{T^k}
<![CDATA[\subsetneq\krn{T^{k+1}}<m> for </m>0\leq k\leq p-1<m>.  If </m>\krn{T^k}=\krn{T^{k+1}}<m> for some </m>k<p<m>, then </m>\krn{T^k}=\krn{T^p}=V<m>.  This implies that </m>T^k=0<m>, violating the fact that </m>T<m> has index </m>p<m>.  So the smallest value of </m>m<m> is indeed </m>p<m>, and we learn that </m>p<n$.]]>
</proof>
</theorem>

The structure of the kernels of powers of nilpotent linear transformations will be crucial to what follows.  But immediately we can see a practical benefit.  Suppose we are confronted with the question of whether or not an <m>n\times n</m> matrix, <m>A</m>, is nilpotent or not.  If we don't quickly find a low power that equals the zero matrix, when do we stop trying higher and higher powers?  <acroref type="theorem" acro="KPNLT" /> gives us the answer: if we don't see a zero matrix by the time we finish computing <m>A^n</m>, then it is not going to ever happen.
We'll now take a look at one example of <acroref type="theorem" acro="KPNLT" /> in action.
<example acro="KPNLT" index="linear transformation!nilpotent!kernels">
<title>Kernels of powers of a nilpotent linear transformation</title>

We will recycle the nilpotent matrix <m>A</m> of index 4 from <acroref type="example" acro="NM64" />.  We now know that would have only needed to look at the first 6 powers of <m>A</m> if the matrix had not been nilpotent.  We list bases for the null spaces of the powers of <m>A</m>.  (Notice how we are using null spaces for matrices interchangeably with kernels of linear transformations, see <acroref type="theorem" acro="KNSI" /> for justification.)
<me><md>
<mrow>
\nsp{A}
</mrow>
<mrow>&amp;=]]>
\nsp{
\begin{bmatrix}
<![CDATA[ -3 & 3 & -2 & 5 & 0 & -5 \\]]>
<![CDATA[ -3 & 5 & -3 & 4 & 3 & -9 \\]]>
<![CDATA[ -3 & 4 & -2 & 6 & -4 & -3 \\]]>
<![CDATA[ -3 & 3 & -2 & 5 & 0 & -5 \\]]>
<![CDATA[ -3 & 3 & -2 & 4 & 2 & -6 \\]]>
<![CDATA[ -2 & 3 & -2 & 2 & 4 & -7]]>
\end{bmatrix}
}
=\spn{\set{
\colvector{2 \\ 2 \\ 5 \\ 2 \\ 1 \\ 0},\,
\colvector{-1 \\ -1 \\ -5 \\ -1 \\ 0 \\ 1}
}}\\
\nsp{A^2}
</mrow>
<mrow>&amp;=]]>
\nsp{
\begin{bmatrix}
<![CDATA[ 1 & -2 & 1 & 0 & -3 & 4 \\]]>
<![CDATA[ 0 & -2 & 1 & 1 & -3 & 4 \\]]>
<![CDATA[ 3 & 0 & 0 & -3 & 0 & 0 \\]]>
<![CDATA[ 1 & -2 & 1 & 0 & -3 & 4 \\]]>
<![CDATA[ 0 & -2 & 1 & 1 & -3 & 4 \\]]>
<![CDATA[ -1 & -2 & 1 & 2 & -3 & 4]]>
\end{bmatrix}
}
=\spn{\set{
\colvector{0 \\ 1 \\ 2 \\ 0 \\ 0 \\ 0},\,
\colvector{2 \\ 1 \\  0 \\ 2 \\ 0 \\ 0},\,
\colvector{0 \\ -3 \\ 0 \\ 0 \\ 2 \\ 0},\,
\colvector{0 \\ 2 \\ 0 \\ 0 \\ 0 \\ 1}
}}\\
\nsp{A^3}
</mrow>
<mrow>&amp;=]]>
\nsp{
\begin{bmatrix}
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0 \\]]>
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0 \\]]>
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0 \\]]>
<![CDATA[ 1 & 0 & 0 & -1 & 0 & 0]]>
\end{bmatrix}
}
=\spn{\set{
\colvector{0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0},\,
\colvector{0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0},\,
\colvector{1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0},\,
\colvector{0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0},\,
\colvector{0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1}
}}\\
\nsp{A^4}
</mrow>
<mrow>&amp;=]]>
\nsp{
\begin{bmatrix}
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}
}
=\spn{\set{
\colvector{1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0},\,
\colvector{0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0},\,
\colvector{0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0},\,
\colvector{0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0},\,
\colvector{0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0},\,
\colvector{0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1}
}}\\
</mrow>
</md></me>
With the exception of some convenience scaling of the basis vectors in <m>\nsp{A^2}</m> these are exactly the basis vectors described in <acroref type="theorem" acro="BNS" />.  We can see that the dimension of <m>\nsp{A}</m> equals the geometric multiplicity of the zero eigenvalue.  Why is this not an accident?  We can see the dimensions of the kernels consistently increasing, and we can see that <m>\nsp{A^4}=\complex{6}</m>.  But  <acroref type="theorem" acro="KPNLT" /> says a little more.  Each successive kernel should be a superset of the previous one.  We ought to be able to begin with a basis of <m>\nsp{A}</m> and extend it to a basis of <m>\nsp{A^2}</m>.  Then we should be able to extend a basis of  <m>\nsp{A^2}</m> into a basis of <m>\nsp{A^3}</m>, all with repeated applications of <acroref type="theorem" acro="ELIS" />.  Verify the following,
<me><md>
<mrow>
\nsp{A}
</mrow>
<mrow>&amp;=]]>
\spn{\set{
\colvector{2 \\ 2 \\ 5 \\ 2 \\ 1 \\ 0},\,
\colvector{-1 \\ -1 \\ -5 \\ -1 \\ 0 \\ 1}
}}\\
\nsp{A^2}
</mrow>
<mrow>&amp;=\spn{\set{]]>
\colvector{2 \\ 2 \\ 5 \\ 2 \\ 1 \\ 0},\,
\colvector{-1 \\ -1 \\ -5 \\ -1 \\ 0 \\ 1},\,
\colvector{0 \\ -3 \\ 0 \\ 0 \\ 2 \\ 0},\,
\colvector{0 \\ 2 \\ 0 \\ 0 \\ 0 \\ 1}
}}\\
\nsp{A^3}
</mrow>
<mrow>&amp;=]]>
\spn{\set{
\colvector{2 \\ 2 \\ 5 \\ 2 \\ 1 \\ 0},\,
\colvector{-1 \\ -1 \\ -5 \\ -1 \\ 0 \\ 1},\,
\colvector{0 \\ -3 \\ 0 \\ 0 \\ 2 \\ 0},\,
\colvector{0 \\ 2 \\ 0 \\ 0 \\ 0 \\ 1},\,
\colvector{0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1}
}}\\
\nsp{A^4}
</mrow>
<mrow>&amp;=]]>
\spn{\set{
\colvector{2 \\ 2 \\ 5 \\ 2 \\ 1 \\ 0},\,
\colvector{-1 \\ -1 \\ -5 \\ -1 \\ 0 \\ 1},\,
\colvector{0 \\ -3 \\ 0 \\ 0 \\ 2 \\ 0},\,
\colvector{0 \\ 2 \\ 0 \\ 0 \\ 0 \\ 1},\,
\colvector{0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1},\,
\colvector{0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0}
}}\\
</mrow>
</md></me>
Do not be concerned at the moment about how these bases were constructed since we are not describing the applications of <acroref type="theorem" acro="ELIS" /> here.  Do verify carefully for each alleged basis that, (1) it is a superset of the basis for the previous kernel, (2) the basis vectors really are members of the kernel of the right power of <m>A</m>, (3) the basis is a linearly independent set, (4) the size of the basis is equal to the size of the basis found previously for each kernel.  With these verifications, <acroref type="theorem" acro="G" /> will tell us that we have successfully demonstrated what <acroref type="theorem" acro="KPNLT" /> guarantees.
</example>

\subsect{CFNLT}{Canonical Form for Nilpotent Linear Transformations}
Our main purpose in this section is to find a basis so that a nilpotent linear transformation will have a pleasing, nearly-diagonal matrix representation.  Of course, we will not have a definition for <q>pleasing,</q> nor for <q>nearly-diagonal.</q>  But the short answer is that our preferred matrix representation will be built up from Jordan blocks, <m>\jordan{n}{0}</m>.  Here's the theorem.  You will find <acroref type="example" acro="CFNLT" /> helpful as you study this proof, since it uses the same notation, and is large enough to (barely) illustrate the full generality of the theorem.
<theorem acro="CFNLT" index="canonical form!nilpotent linear transformation">
<title>Canonical Form for Nilpotent Linear Transformations</title>
<statement>
Suppose that <m>\ltdefn{T}{V}{V}</m> is a nilpotent linear transformation of index <m>p</m>.  Then there is a basis for <m>V</m> so that the matrix representation, <m>\matrixrep{T}{B}{B}</m>, is block diagonal with each block being a Jordan block, <m>\jordan{n}{0}</m>.  The size of the largest block is the index <m>p</m>, and the total number of blocks is the nullity of <m>T</m>, <m>\nullity{T}</m>.
</statement>

<proof>
We will explicitly construct the desired basis, so the proof is constructive (<acroref type="technique" acro="C" />), and can be used in practice.  As we begin, the basis vectors will not be in the proper order, but we will rearrange them at the end of the proof.  For convenience, define <m>n_i=\nullity{T^i}</m>, so for example, <m>n_0=0</m>, <m>n_1=\nullity{T}</m> and <m>n_p=\nullity{T^p}=\dimension{V}</m>.  Define <m>s_i=n_i-n_{i-1}</m>, for <m>1\leq i\leq p</m>, so we can think of <m>s_i</m> as <q>how much bigger</q> <m>\krn{T^i}</m> is than <m>\krn{T^{i-1}}</m>.  In particular, <acroref type="theorem" acro="KPNLT" /> implies that <m>s_i>0</m> for <m>1\leq i\leq p</m>.<p><p>
We are going to build a set of vectors <m>\vect{z}_{i,j}</m>, <m>1\leq i\leq p</m>, <m>1\leq j\leq s_i</m>.  Each <m>\vect{z}_{i,j}</m> will be an element of <m>\krn{T^i}</m> and not an element of <m>\krn{T^{i-1}}</m>.  In total, we will obtain a linearly independent set of <m>\sum_{i=1}^{p}s_i=\sum_{i=1}^{p}n_i-n_{i-1}=n_p-n_0=\dimension{V}</m> vectors that form a basis of <m>V</m>.  We construct this set in pieces, starting at the <q>wrong</q> end.  Our procedure will build a series of subspaces, <m>Z_i</m>, each lying in between  <m>\krn{T^{i-1}}</m> and <m>\krn{T^i}</m>, having bases <m>\vect{z}_{i,j}</m>, <m>1\leq j\leq s_i</m>, and which together equal <m>V</m> as a direct sum.  Now would be a good time to review the results on direct sums collected in <acroref type="subsection" acro="PD.DS" />.   OK, here we go.<p><p>
We build the subspace <m>Z_p</m> first (this is what we meant by <q>starting at the wrong end</q>).  <m>\krn{T^{p-1}}</m> is a proper subspace of <m>\krn{T^p}=V</m> (<acroref type="theorem" acro="KPNLT" />).  <acroref type="theorem" acro="DSFOS" /> says that there is a subspace of <m>V</m> that will pair with the subspace <m>\krn{T^{p-1}}</m> to form a direct sum of <m>V</m>.  Call this subspace <m>Z_p</m>, and choose vectors <m>\vect{z}_{p,j}</m>, <m>1\leq j\leq s_p</m> as a basis of <m>Z_p</m>, which we will denote as <m>B_p</m>.   Note that we have a fair amount of freedom in how to choose these first basis vectors.  Several observations will be useful in the next step.  First <m>V=\krn{T^{p-1}}\ds Z_p</m>.  The basis <m>B_p=\set{\vect{z}_{p,1},\,\vect{z}_{p,2},\,\vect{z}_{p,3},\,\dots,\,\vect{z}_{p,s_p}}</m> is linearly independent.  For <m>1\leq j\leq s_p</m>, <m>\vect{z}_{p,j}\in\krn{T^p}=V</m>.  Since the two subspaces of a direct sum have no nonzero vectors in common (<acroref type="theorem" acro="DSZI" />), for <m>1\leq j\leq s_p</m>, <m>\vect{z}_{p,j}\not\in\krn{T^{p-1}}</m>.  That was comparably easy.<p><p>
If obtaining <m>Z_p</m> was easy, getting <m>Z_{p-1}</m> will be harder.  We will repeat the next step <m>p-1</m> times, and so will do it carefully the first time.  Eventually, <m>Z_{p-1}</m> will have dimension <m>s_{p-1}</m>.  However, the first <m>s_p</m> vectors of a basis are straightforward.  Define <m>\vect{z}_{p-1,j}=\lt{T}{\vect{z}_{p,j}}</m>, <m>1\leq j\leq s_p</m>.  Notice that we have no choice in creating these vectors, they are a consequence of our choices for <m>\vect{z}_{p,j}</m>.  In retrospect (<ie /> on a second reading of this proof), you will recognize  this as the key step in realizing a matrix representation of a nilpotent linear transformation with Jordan blocks.  We need to know that this set of vectors in linearly independent, so start with a relation of linear dependence (<acroref type="definition" acro="RLD" />), and massage it,
<me><md>
<mrow>
\zerovector
</mrow>
<mrow>&amp;=]]>
a_1\vect{z}_{p-1,1}+
a_2\vect{z}_{p-1,2}+
a_3\vect{z}_{p-1,3}+
\cdots+
a_{s_p}\vect{z}_{p-1,s_p}\\
</mrow>
<mrow>&amp;=]]>
a_1\lt{T}{\vect{z}_{p,1}}+
a_2\lt{T}{\vect{z}_{p,2}}+
a_3\lt{T}{\vect{z}_{p,3}}+
\cdots+
a_{s_p}\lt{T}{\vect{z}_{p,s_p}}\\
</mrow>
<mrow>&amp;=\lt{T}{]]>
a_1\vect{z}_{p,1}+
a_2\vect{z}_{p,2}+
a_3\vect{z}_{p,3}+
\cdots+
a_{s_p}\vect{z}_{p,s_p}
}
</mrow>
<mrow>&amp;&]]><acroref type="theorem" acro="LTLC" />
</mrow>
</md></me>
Define
$\vect{x}=
a_1\vect{z}_{p,1}+a_2\vect{z}_{p,2}+a_3\vect{z}_{p,3}+\cdots+a_{s_p}\vect{z}_{p,s_p}$.
The statement just above means that <m>\vect{x}\in\krn{T}\subseteq\krn{T^{p-1}}</m>  (<acroref type="definition" acro="KLT" />, <acroref type="theorem" acro="KPNLT" />).  As defined, <m>\vect{x}</m> is a linear combination of the basis vectors <m>B_p</m>, and therefore <m>\vect{x}\in Z_p</m>.  Thus <m>\vect{x}\in\krn{T^{p-1}}\cap Z_p</m> (<acroref type="definition" acro="SI" />).  Because <m>V=\krn{T^{p-1}}\ds Z_p</m>, <acroref type="theorem" acro="DSZI" /> tells us that <m>\vect{x}=\zerovector</m>.  Now we recognize the definition of <m>\vect{x}</m> as a relation of linear dependence on the linearly independent set <m>B_p</m>, and therefore <m>a_1=a_2=\cdots=a_{s_p}=0</m> (<acroref type="definition" acro="LI" />). This establishes the linear independence of <m>\vect{z}_{p-1,j}</m>, <m>1\leq j\leq s_p</m> (<acroref type="definition" acro="LI" />).<p><p>
We also need to know where the vectors <m>\vect{z}_{p-1,j}</m>, <m>1\leq j\leq s_p</m> live.  First we demonstrate that they are members of <m>\krn{T^{p-1}}</m>.
<me><md>
<mrow>
\lt{T^{p-1}}{\vect{z}_{p-1,j}}
</mrow>
<mrow>&amp;=\lt{T^{p-1}}{\lt{T}{\vect{z}_{p,j}}}\\]]>
</mrow>
<mrow>&amp;=\lt{T^{p}}{\vect{z}_{p,j}}\\]]>
</mrow>
<mrow>&amp;=\zerovector]]>
</mrow>
</md></me>
So <m>\vect{z}_{p-1,j}\in\krn{T^{p-1}}</m>, <m>1\leq j\leq s_p</m>.  However, we now show that these vectors are not elements of <m>\krn{T^{p-2}}</m>.  Suppose to the contrary (<acroref type="technique" acro="CD" />) that <m>\vect{z}_{p-1,j}\in\krn{T^{p-2}}</m>.  Then
<me><md>
<mrow>
\zerovector
</mrow>
<mrow>&amp;=\lt{T^{p-2}}{\vect{z}_{p-1,j}}\\]]>
</mrow>
<mrow>&amp;=\lt{T^{p-2}}{\lt{T}{\vect{z}_{p,j}}}\\]]>
</mrow>
<mrow>&amp;=\lt{T^{p-1}}{\vect{z}_{p,j}}\\]]>
</mrow>
</md></me>
which contradicts the earlier statement that <m>\vect{z}_{p,j}\not\in\krn{T^{p-1}}</m>.  So <m>\vect{z}_{p-1,j}\not\in\krn{T^{p-2}}</m>, <m>1\leq j\leq s_p</m>.<p><p>
Now choose a basis <m>C_{p-2}=\set{\vectorlist{u}{n_{p-2}}}</m> for <m>\krn{T^{p-2}}</m>.  We want to extend this basis by adding in the <m>\vect{z}_{p-1,j}</m> to span a subspace of <m>\krn{T^{p-1}}</m>.  But first we want to know that this set is linearly independent.  Let <m>a_k</m>, <m>1\leq k\leq n_{p-2}</m> and <m>b_j</m>, <m>1\leq j\leq s_p</m> be the scalars in a relation of linear dependence,
<me><md>
<mrow>
\zerovector
</mrow>
<mrow>&amp;=]]>
a_1\vect{u}_1+
a_2\vect{u}_2+
\cdots+
a_{n_{p-2}}\vect{u}_{n_{p-2}}
+
b_1\vect{z}_{p-1,1}+
b_2\vect{z}_{p-1,2}+
\cdots+
b_{s_p}\vect{z}_{p-1,s_p}
</mrow>
</md></me>
Then,
<me><md>
<mrow>
\zerovector
</mrow>
<mrow>&amp;=\lt{T^{p-2}}{\zerovector}\\]]>
</mrow>
<mrow>&amp;=\lt{T^{p-2}}{]]>
a_1\vect{u}_1+
a_2\vect{u}_2+
\cdots+
a_{n_{p-2}}\vect{u}_{n_{p-2}}
+
b_1\vect{z}_{p-1,1}+
b_2\vect{z}_{p-1,2}+
\cdots+
b_{s_p}\vect{z}_{p-1,s_p}
}\\
</mrow>
<mrow>&amp;=]]>
a_1\lt{T^{p-2}}{\vect{u}_1}+
a_2\lt{T^{p-2}}{\vect{u}_2}+
\cdots+
a_{n_{p-2}}\lt{T^{p-2}}{\vect{u}_{n_{p-2}}}
+\\
</mrow>
<mrow>&amp;\quad\quad]]>
b_1\lt{T^{p-2}}{\vect{z}_{p-1,1}}+
b_2\lt{T^{p-2}}{\vect{z}_{p-1,2}}+
\cdots+
b_{s_p}\lt{T^{p-2}}{\vect{z}_{p-1,s_p}}\\
</mrow>
<mrow>&amp;=]]>
a_1\zerovector+
a_2\zerovector+
\cdots+
a_{n_{p-2}}\zerovector
+
b_1\lt{T^{p-2}}{\vect{z}_{p-1,1}}+
b_2\lt{T^{p-2}}{\vect{z}_{p-1,2}}+
\cdots+
b_{s_p}\lt{T^{p-2}}{\vect{z}_{p-1,s_p}}\\
</mrow>
<mrow>&amp;=]]>
b_1\lt{T^{p-2}}{\vect{z}_{p-1,1}}+
b_2\lt{T^{p-2}}{\vect{z}_{p-1,2}}+
\cdots+
b_{s_p}\lt{T^{p-2}}{\vect{z}_{p-1,s_p}}\\
</mrow>
<mrow>&amp;=]]>
b_1\lt{T^{p-2}}{\lt{T}{\vect{z}_{p,1}}}+
b_2\lt{T^{p-2}}{\lt{T}{\vect{z}_{p,2}}}+
\cdots+
b_{s_p}\lt{T^{p-2}}{\lt{T}{\vect{z}_{p,s_p}}}\\
</mrow>
<mrow>&amp;=]]>
b_1\lt{T^{p-1}}{\vect{z}_{p,1}}+
b_2\lt{T^{p-1}}{\vect{z}_{p,2}}+
\cdots+
b_{s_p}\lt{T^{p-1}}{\vect{z}_{p,s_p}}\\
</mrow>
<mrow>&amp;=]]>
\lt{T^{p-1}}{
b_1\vect{z}_{p,1}+
b_2\vect{z}_{p,2}+
\cdots+
b_{s_p}\vect{z}_{p,s_p}
}
</mrow>
</md></me>
Define
$\vect{y}=
b_1\vect{z}_{p,1}+b_2\vect{z}_{p,2}+\cdots+b_{s_p}\vect{z}_{p,s_p}$.
The statement just above means that <m>\vect{y}\in\krn{T^{p-1}}</m>  (<acroref type="definition" acro="KLT" />).  As defined, <m>\vect{y}</m> is a linear combination of the basis vectors <m>B_p</m>, and therefore <m>\vect{y}\in Z_p</m>.  Thus <m>\vect{y}\in\krn{T^{p-1}}\cap Z_p</m>.  Because <m>V=\krn{T^{p-1}}\ds Z_p</m>, <acroref type="theorem" acro="DSZI" /> tells us that <m>\vect{y}=\zerovector</m>.  Now we recognize the definition of <m>\vect{y}</m> as a relation of linear dependence on the linearly independent set <m>B_p</m>, and therefore <m>b_1=b_2=\cdots=b_{s_p}=0</m> (<acroref type="definition" acro="LI" />).  Return to the full relation of linear dependence with both sets of scalars (the <m>a_i</m> and <m>b_j</m>).  Now that we know that <m>b_j=0</m> for <m>1\leq j\leq s_p</m>, this relation of linear dependence simplifies to a relation of linear dependence on just the basis <m>C_{p-1}</m>.  Therefore, <m>a_i=0</m>, <m>1\leq a_i\leq n_{p-1}</m> and we have the desired linear independence.<p><p>
Define a new subspace of <m>\krn{T^{p-1}}</m> as
<me><md>
<mrow>
Q_{p-1}
</mrow>
<mrow>&amp;=]]>
\spn{\set{
\vectorlist{u}{n_{p-1}},\,
\vect{z}_{p-1,1},\,\vect{z}_{p-1,2},\,\vect{z}_{p-1,3},\,\dots,\,\vect{z}_{p-1,s_p}
}}
</mrow>
</md></me>
By <acroref type="theorem" acro="DSFOS" /> there exists a subspace of <m>\krn{T^{p-1}}</m> which will pair with <m>Q_{p-1}</m> to form a direct sum.  Call this subspace <m>R_{p-1}</m>, so by definition, <m>\krn{T^{p-1}}=Q_{p-1}\ds R_{p-1}</m>.  We are interested in the dimension of <m>R_{p-1}</m>.  Note first, that since the spanning set of <m>Q_{p-1}</m> is linearly independent, <m>\dimension{Q_{p-1}}=n_{p-2}+s_p</m>.  Then
<me><md>
<mrow>
\dimension{R_{p-1}}
</mrow>
<mrow>&amp;=\dimension{\krn{T^{p-1}}}-\dimension{Q_{p-1}}&amp;&amp;<acroref type="theorem" acro="DSD" />
</mrow>
<mrow>&amp;=n_{p-1}-\left(n_{p-2}+s_p\right)\\]]>
</mrow>
<mrow>&amp;=\left(n_{p-1}-n_{p-2}\right)-s_p\\]]>
</mrow>
<mrow>&amp;=s_{p-1}-s_p]]>
</mrow>
</md></me>
Notice that if <m>s_{p-1}=s_p</m>, then <m>R_{p-1}</m> is trivial.   Now choose a basis of <m>R_{p-1}</m>, and denote these <m>s_{p-1}-s_p</m> vectors as <m>\vect{z}_{p-1,s_p+1}</m>, <m>\vect{z}_{p-1,s_p+2}</m>, <m>\vect{z}_{p-1,s_p+3}</m>, \dots, <m>\vect{z}_{p-1,s_{p-1}}</m>.  This is another occassion to notice that we have some freedom in this choice.<p><p>
We now have <m>\krn{T^{p-1}}=Q_{p-1}\ds R_{p-1}</m>, and we have bases for each of the two subspaces.  The union of these two bases will therefore be a linearly independent set in <m>\krn{T^{p-1}}</m> with size
<me><md>
<mrow>
\left(n_{p-2}+s_p\right)+\left(s_{p-1}-s_p\right)
</mrow>
<mrow>&amp;=n_{p-2}+s_{p-1}\\]]>
</mrow>
<mrow>&amp;=n_{p-2}+n_{p-1}-n_{p-2}\\]]>
</mrow>
<mrow>&amp;=n_{p-1}=\dimension{\krn{T^{p-1}}}]]>
</mrow>
</md></me>
So, by <acroref type="theorem" acro="G" />, the following set is a basis of <m>\krn{T^{p-1}}</m>,
<me><md>
<mrow>
\set{
\vectorlist{u}{n_{p-2}},\,
\vect{z}_{p-1,1},\,\vect{z}_{p-1,2},\,\dots,\,\vect{z}_{p-1,s_p},\,
\vect{z}_{p-1,s_p+1},\,\vect{z}_{p-1,s_p+2},\,\dots,\,\vect{z}_{p-1,s_{p-1}}
}
</mrow>
</md></me>
We built up this basis in three parts, we will now split it in half.  Define the subspace <m>Z_{p-1}</m> by
<me><md>
<mrow>
Z_{p-1}
</mrow>
<mrow>&amp;=\spn{B_{p-1}}=]]>
\spn{\set{
\vect{z}_{p-1,1},\,\vect{z}_{p-1,2},\,\dots,\,\vect{z}_{p-1,s_{p-1}}
}}
</mrow>
</md></me>
where we have implicitly denoted the basis as <m>B_{p-1}</m>.  Then <acroref type="theorem" acro="DSFB" /> allows us to split up the basis for <m>\krn{T^{p-1}}</m> as <m>C_{p-1}\cup B_{p-1}</m> and write
<me><md>
<mrow>
\krn{T^{p-1}}=\krn{T^{p-2}}\ds Z_{p-1}
</mrow>
</md></me>
Whew!  This is a good place to recap what we have achieved.  The vectors <m>\vect{z}_{i,j}</m> form bases for the subspaces <m>Z_i</m> and right now
<me><md>
<mrow>
V=\krn{T^{p-1}}\ds Z_{p}=\krn{T^{p-2}}\ds Z_{p-1}\ds Z_{p}
</mrow>
</md></me>
The key feature of this decomposition of <m>V</m> is that the first <m>s_p</m> vectors in the basis for <m>Z_{p-1}</m> are outputs of the linear transformation <m>T</m> using the basis vectors of <m>Z_p</m> as inputs.<p><p>
Now we want to further decompose <m>\krn{T^{p-2}}</m> (into <m>\krn{T^{p-3}}</m> and <m>Z_{p-2}</m>).  The procedure is the same as above, so we will only sketch the key steps.  Checking the details proceeds in the same manner as above.  Technically, we could have set up the preceding as the induction step in a proof by induction (<acroref type="technique" acro="I" />), but this probably would make the proof harder to understand.<p><p>
Hit each element of <m>B_{p-1}</m> with <m>T</m>, to create vectors <m>\vect{z}_{p-2,j}</m>, <m>1\leq j\leq s_{p-1}</m>.  These vectors form a linearly independent set, and each is an element of <m>\krn{T^{p-2}}</m>, but not an element of <m>\krn{T^{p-3}}</m>.  Grab a basis <m>C_{p-3}</m> of <m>\krn{T^{p-3}}</m> and tack on the newly-created vectors <m>\vect{z}_{p-2,j}</m>, <m>1\leq j\leq s_{p-1}</m>.  This expanded set is linearly independent, and we can define a subspace <m>Q_{p-2}</m> using it as a basis.  <acroref type="theorem" acro="DSFOS" /> gives us a subspace <m>R_{p-2}</m> such that <m>\krn{T^{p-2}}=Q_{p-2}\ds R_{p-2}</m>.  Vectors <m>\vect{z}_{p-2,j}</m>, <m>s_{p-1}+1\leq j\leq s_{p-2}</m> are chosen as a basis for <m>R_{p-2}</m> once the relevant dimensions have been verified.  The union of <m>C_{p-3}</m> and <m>\vect{z}_{p-2,j}</m>, <m>1\leq j\leq s_{p-2}</m> then form a basis of <m>\krn{T^{p-2}}</m>, which can be split into two parts to yield the decomposition
<me><md>
<mrow>
\krn{T^{p-2}}=\krn{T^{p-3}}\ds Z_{p-2}
</mrow>
</md></me>
Here <m>Z_{p-2}</m> is the subspace of <m>\krn{T^{p-2}}</m> with basis <m>B_{p-2}=\setparts{\vect{z}_{p-2,j}}{1\leq j\leq s_{p-2}}</m>.  Finally,
<me><md>
<mrow>
V=\krn{T^{p-1}}\ds Z_{p}=\krn{T^{p-2}}\ds Z_{p-1}\ds Z_{p}=\krn{T^{p-3}}\ds Z_{p-2}\ds Z_{p-1}\ds Z_{p}
</mrow>
</md></me>
Again, the key feature of this decomposition is that the first vectors in the basis of <m>Z_{p-2}</m> are outputs of <m>T</m> using vectors from the basis <m>Z_{p-1}</m> as inputs (and in turn, some of these inputs are outputs of <m>T</m> derived from inputs in <m>Z_p</m>).<p><p>
Now assume we repeat this procedure until we decompose <m>\krn{T^{2}}</m> into subspaces <m>\krn{T}</m> and <m>Z_2</m>.  Finally, decompose <m>\krn{T}</m> into subspaces <m>\krn{T^0}=\krn{I_n}=\set{\zerovector}</m> and <m>Z_1</m>, so that we recognize the vectors <m>\vect{z}_{1,j}</m>, <m>1\leq j\leq s_1=n_1</m> as elements of <m>\krn{T}</m>.  The set
<me><md>
<mrow>
B
</mrow>
<mrow>&amp;=B_1\cup B_2\cup B_3\cup\cdots\cup B_p]]>
=\setparts{\vect{z}_{i,j}}{1\leq i\leq p,\ 1\leq j\leq s_i}
</mrow>
</md></me>
is linearly independent by <acroref type="theorem" acro="DSLI" /> and has size
<me><md>
<mrow>
\sum_{i=1}^{p}s_i=\sum_{i=1}^{p}n_i-n_{i-1}=n_p-n_0=\dimension{V}
</mrow>
</md></me>
So by <acroref type="theorem" acro="G" />, <m>B</m> is a basis of <m>V</m>.  We desire a matrix representation of <m>T</m> relative to <m>B</m> (<acroref type="definition" acro="MR" />), but first we will reorder the elements of <m>B</m>.  The following display lists the elements of <m>B</m> in the desired order, when read across the rows left-to-right in the usual way.  Notice that we established the existence of these vectors column-by-column, and beginning on the right.
<me><md>
<mrow>
</mrow>
<mrow>&amp;\vect{z}_{1,1}&&\vect{z}_{2,1}&&\vect{z}_{3,1}&&\cdots&&\vect{z}_{p,1}\\]]>
</mrow>
<mrow>&amp;\vect{z}_{1,2}&&\vect{z}_{2,2}&&\vect{z}_{3,2}&&\cdots&&\vect{z}_{p,2}\\]]>
</mrow>
<mrow>&amp;                       &&\vdots              &&                       &&\vdots&&\\]]>
</mrow>
<mrow>&amp;\vect{z}_{1,s_p}&&\vect{z}_{2,s_p}&&\vect{z}_{3,s_p}&&\cdots&&\vect{z}_{p,s_p}\\]]>
</mrow>
<mrow>&amp;\vect{z}_{1,s_p+1}&&\vect{z}_{2,s_p+1}&&\vect{z}_{3,s_p+1}&&\cdots&&\\]]>
</mrow>
<mrow>&amp;                       &&\vdots              &&                       &&\vdots&&\\]]>
</mrow>
<mrow>&amp;\vect{z}_{1,s_3}&&\vect{z}_{2,s_3}&&\vect{z}_{3,s_3}&&&&\\]]>
</mrow>
<mrow>&amp;                          &&\vdots&&&&&&\\]]>
</mrow>
<mrow>&amp;\vect{z}_{1,s_2}&&\vect{z}_{2,s_2}&&&&&&\\]]>
</mrow>
<mrow>&amp;                          &&\vdots&&&&&&\\]]>
</mrow>
<mrow>&amp;\vect{z}_{1,s_1}&&&&&&&&\\]]>
</mrow>
</md></me>
It is difficult to layout this table with the notation we have been using, but it would not be especially useful to invent some notation to overcome the difficulty. (One approach would be to define something like the inverse of the nonincreasing function, <m>i\rightarrow s_i</m>.)  Do notice that there are <m>s_1=n_1</m> rows and <m>p</m> columns.  Column <m>i</m> is the basis <m>B_i</m>.  The vectors in the first column are elements of <m>\krn{T}</m>.  Each row is the same length, or shorter, than the one above it.  If we apply <m>T</m> to any vector in the table, other than those in the first column, the output is the preceding vector in the row.<p><p>
Now contemplate the matrix representation of <m>T</m> relative to <m>B</m> as we read across the rows of the table above.  In the first row, <m>\lt{T}{\vect{z}_{1,1}}=\zerovector</m>, so the first column of the representation is the zero column.  Next, <m>\lt{T}{\vect{z}_{2,1}}=\vect{z}_{1,1}</m>, so the second column of the representation is a vector with a single one in the first entry, and zeros elsewhere.  Next, <m>\lt{T}{\vect{z}_{3,1}}=\vect{z}_{2,1}</m>, so column 3 of the representation is a zero, then a one, then all zeros.  Continuing in this vein, we obtain the first <m>p</m> columns of the representation, which is the Jordan block <m>\jordan{p}{0}</m> followed by rows of zeros.<p><p>
When we apply <m>T</m> to the basis vectors of the second row, what happens?  Applying <m>T</m> to the first vector, the result is the zero vector, so the representation gets a zero column.  Applying <m>T</m> to the second vector in the row, the output is simply the first vector in that row, making the next column of the representation all zeros plus a lone one, sitting just above the diagonal.  Continuing, we create a Jordan block, sitting on the diagonal of the matrix representation.  It is not possible in general to state the size of this block, but since the second row is no longer than the first, it cannot have size larger than <m>p</m>.<p><p>
Since there are as many rows as the dimension of <m>\krn{T}</m>, the representation contains as many Jordan blocks as the nullity of <m>T</m>, <m>\nullity{T}</m>.  Each successive block is smaller than the preceding one, with the first, and largest, having size <m>p</m>.  The blocks are Jordan blocks since the basis vectors <m>\vect{z}_{i,j}</m> were often defined as the result of applying <m>T</m> to other elements of the basis already determined, and then we rearranged the basis into an order that placed outputs of <m>T</m> just before their inputs, excepting the start of each row, which was an element of <m>\krn{T}</m>.
</proof>
</theorem>

The proof of <acroref type="theorem" acro="CFNLT" /> is constructive (<acroref type="technique" acro="C" />), so we can use it to create bases of nilpotent linear transformations with pleasing matrix representations.  Recall that <acroref type="theorem" acro="DNLT" /> told us that nilpotent linear transformations are almost never diagonalizable, so this is progress.  As we have hinted before, with a nice representation of nilpotent matrices, it will not be difficult to build up representations of other non-diagonalizable matrices.  Here is the promised example which illustrates the previous theorem.  It is a useful companion to your study of the proof of <acroref type="theorem" acro="CFNLT" />.
<example acro="CFNLT" index="canonical form!nilpotent linear transformation">
<title>Canonical form for a nilpotent linear transformation</title>

The <m>6\times 6</m> matrix, <m>A</m>, of <acroref type="example" acro="NM64" /> is nilpotent of index <m>p=4</m>.  If we define the linear transformation <m>\ltdefn{T}{\complex{6}}{\complex{6}}</m> by <m>\lt{T}{\vect{x}}=A\vect{x}</m>, then <m>T</m> is nilpotent of index <m>4</m> and we can seek a basis of <m>\complex{6}</m> that yields a matrix representation with Jordan blocks on the diagonal.  The nullity of <m>T</m> is <m>2</m>, so from <acroref type="theorem" acro="CFNLT" /> we can expect the largest Jordan block to be <m>\jordan{4}{0}</m>, and there will be just two blocks.  This only leaves enough room for the second block to have size 2.<p><p>
We will recycle the bases for the null spaces of the powers of <m>A</m> from <acroref type="example" acro="KPNLT" /> rather than recomputing them here.  We will also use the same notation used in the proof of <acroref type="theorem" acro="CFNLT" />.<p><p>
To begin, <m>s_4=n_4-n_3=6-5=1</m>, so we need one vector of <m>\krn{T^4}=\complex{6}</m>, that is not in <m>\krn{T^3}</m>, to be a basis for <m>Z_4</m>.  We have a lot of latitude in this choice, and we have not described any sure-fire method for constructing a vector <em>outside</em> of a subspace.  Looking at the basis for <m>\krn{T^3}</m> we see that if a vector is in this subspace, and has a nonzero value in the first entry, then it must also have a nonzero value in the fourth entry.  So the vector
<me><md>
<mrow>
\vect{z}_{4,1}=\colvector{1\\0\\0\\0\\0\\0}
</mrow>
</md></me>
will not be an element of <m>\krn{T^3}</m>  (notice that many other choices could be made here, so our basis will not be unique).  This completes the determination of <m>Z_p=Z_4</m>.<p><p>
Next, <m>s_3=n_3-n_2=5-4=1</m>, so we again need just a single basis vector for <m>Z_3</m>.  We start by evaluating <m>T</m> with each basis vector of <m>Z_4</m>,
<me><md>
<mrow>
<![CDATA[\vect{z}_{3,1}&=\lt{T}{\vect{z}_{4,1}}=A\vect{z}_{4,1}=\colvector{-3\\-3\\-3\\-3\\-3\\-2}]]>
</mrow>
</md></me>
Since <m>s_3=s_4</m>, the subspace <m>R_3</m> is trivial, and there is nothing left to do, <m>\vect{z}_{3,1}</m> is the lone basis vector of <m>Z_3</m>.<p><p>
Now <m>s_2=n_2-n_1=4-2=2</m>, so the construction of <m>Z_2</m> will not be as simple as the construction of <m>Z_3</m>.  We first apply <m>T</m> to the basis vector of <m>Z_2</m>,
<me><md>
<mrow>
<![CDATA[\vect{z}_{2,1}&=\lt{T}{\vect{z}_{3,1}}=A\vect{z}_{3,1}=\colvector{1\\0\\3\\1\\0\\-1}]]>
</mrow>
</md></me>
The two basis vectors of <m>\krn{T^1}</m>, together with <m>\vect{z}_{2,1}</m>, form a basis for <m>Q_2</m>.  Because <m>\dimension{\krn{T^2}}-\dimension{Q_2}=4-3=1</m> we need only find a single basis vector for <m>R_2</m>.  This vector must be an element of <m>\krn{T^2}</m>, but not an element of <m>Q_2</m>.  Again, there is a variety of vectors that fit this description, and we have no precise algorithm for finding them.  Since they are plentiful, they are not too hard to find.  We add up the four basis vectors of <m>\krn{T^2}</m>, ensuring an element of <m>\krn{T^2}</m>.  Then we check to see if the vector is a linear combination of three vectors:  the two basis vectors of <m>\krn{T^1}</m> and <m>\vect{z}_{2,1}</m>.  Having passed the tests, we have chosen
<me><md>
<mrow>
<![CDATA[\vect{z}_{2,2}&=\colvector{2\\1\\2\\2\\2\\1}]]>
</mrow>
</md></me>
Thus, <m>Z_2=\spn{\set{\vect{z}_{2,1},\,\vect{z}_{2,2}}}</m>.<p><p>
Lastly, <m>s_1=n_1-n_0=2-0=2</m>.  Since <m>s_2=s_1</m>, we again have a trivial <m>R_1</m> and need only complete our basis by evaluating the basis vectors of <m>Z_2</m> with <m>T</m>,
<me><md>
<mrow>
<![CDATA[\vect{z}_{1,1}&=\lt{T}{\vect{z}_{2,1}}=A\vect{z}_{2,1}=\colvector{1\\1\\0\\1\\1\\1}\\]]>
<![CDATA[\vect{z}_{1,2}&=\lt{T}{\vect{z}_{2,2}}=A\vect{z}_{2,2}=\colvector{-2\\-2\\-5\\-2\\-1\\0}\\]]>
</mrow>
</md></me>
Now we reorder these vectors as the desired basis,
<me><md>
<mrow>
B
</mrow>
<mrow>&amp;=]]>
\set{
\vect{z}_{1,1},\,\vect{z}_{2,1},\,\vect{z}_{3,1},\,\vect{z}_{4,1},\,\vect{z}_{1,2},\,\vect{z}_{2,2}
}
</mrow>
</md></me>
We now apply <acroref type="definition" acro="MR" /> to build a matrix representation of <m>T</m> relative to <m>B</m>,
<me><md>
<mrow>
\vectrep{B}{\lt{T}{\vect{z}_{1,1}}}
</mrow>
<mrow>&amp;=\vectrep{B}{A\vect{z}_{1,1}}=\vectrep{B}{\zerovector}=\colvector{0\\0\\0\\0\\0\\0}\\]]>
\vectrep{B}{\lt{T}{\vect{z}_{2,1}}}
</mrow>
<mrow>&amp;=\vectrep{B}{A\vect{z}_{2,1}}=\vectrep{B}{\vect{z}_{1,1}}=\colvector{1\\0\\0\\0\\0\\0}\\]]>
\vectrep{B}{\lt{T}{\vect{z}_{3,1}}}
</mrow>
<mrow>&amp;=\vectrep{B}{A\vect{z}_{3,1}}=\vectrep{B}{\vect{z}_{2,1}}=\colvector{0\\1\\0\\0\\0\\0}\\]]>
\vectrep{B}{\lt{T}{\vect{z}_{4,1}}}
</mrow>
<mrow>&amp;=\vectrep{B}{A\vect{z}_{4,1}}=\vectrep{B}{\vect{z}_{3,1}}=\colvector{0\\0\\1\\0\\0\\0}\\]]>
\vectrep{B}{\lt{T}{\vect{z}_{1,2}}}
</mrow>
<mrow>&amp;=\vectrep{B}{A\vect{z}_{1,2}}=\vectrep{B}{\zerovector}=\colvector{0\\0\\0\\0\\0\\0}\\]]>
\vectrep{B}{\lt{T}{\vect{z}_{2,2}}}
</mrow>
<mrow>&amp;=\vectrep{B}{A\vect{z}_{2,2}}=\vectrep{B}{\vect{z}_{1,2}}=\colvector{0\\0\\0\\0\\1\\0}\\]]>
</mrow>
</md></me>
Installing these vectors as the columns of the matrix representation we have
<me><md>
<mrow>
<![CDATA[\matrixrep{T}{B}{B}&=]]>
\begin{bmatrix}
<![CDATA[ 0 & 1 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 1 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 1 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 1 \\]]>
<![CDATA[ 0 & 0 & 0 & 0 & 0 & 0]]>
\end{bmatrix}
</mrow>
</md></me>
which is a block diagonal matrix with Jordan blocks <m>\jordan{4}{0}</m> and <m>\jordan{2}{0}</m>.   If we constructed the matrix <m>S</m> having the vectors of <m>B</m> as columns, then <acroref type="theorem" acro="SCB" /> tells us that a similarity transformation with <m>S</m> relates the original matrix representation of <m>T</m> with the matrix representation consisting of Jordan blocks., <ie /> <m>\similar{A}{S}=\matrixrep{T}{B}{B}</m>.
</example>

Notice that constructing interesting examples of matrix representations requires domains with dimensions bigger than just two or three.  Going forward we will see several more big examples.
<!--   End  NLT.tex -->
</subsection>
</section>
